{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking Netflix By Storm: The Correlation between Watching and the Weather\n",
    "\n",
    "TODO:\n",
    "* Seperate code into utils/project specific\n",
    "* More Data Aggregation\n",
    "    * Data dispersion display for each temp category\n",
    "    * Precipitation Hypo Test /\n",
    "    * A third hypothesis test -- ideas on what to test?   \n",
    "* Classification\n",
    "    * kNN\n",
    "    * kNN comparison\n",
    "* Conclusion\n",
    "* Create README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### A. The Entertainment Domain\n",
    "My project will take place in the entertainment domain and deal primarily with my Netflix consumption patterns. Netflix and TV shows in general are certainly prevelant in my daily life. In general, I try to be careful about the amount of time I spend watching Netflix and consuming other digital media in order to ensure that I am not forgetting to live my life and that I am not getting addicted to watching certain shows. The entertainment domain is something that I am continuously aware of and trying to track in my everday life. It is certainly important for society as a whole to be looking into this domain, looking at the factors that impact our consumption within this domain, and reflecting on the impact that this domain has on our lives. I am reasearching this domain because I am particularly interested in finding out if factors outside of this domain, in this case weather, can have a significant impact on how much time we spend within the entertainment domain.  \n",
    "\n",
    "\n",
    "### B. Hypotheses\n",
    "My hypothesis about this domain is that I will find a correlation between my Netflix viewing habits and the weather. I believe that when the weather is colder, then my total time watching Netflix is above average. In addition to this, I also believe the opposite. When the weather is warmer, my total time watching netflix is below my average. The impact of these results could help me in the long run as I try to be conscious and limit the amount of time I spend on Netflix. If I am aware that colder weather has an impact on my viewing patterns, it will help me to remember to be more conscious about it on days when the weather is colder. My roommates are also stakeholders in this study due to the fact that I force them to watch Netflix with me in our apartment. If it is true that the weather effects our viewing patterns, then it is effecting my roommates as well as myself. Netflix as a company is also a stakeholder in this. If there are more people using their platform on a given day they need to make sure that everything is running smoothly and quickly. However, on a warmer day when not as many people are watching, it is not as likely that there will be problems on their platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "### A. Dataset Description\n",
    "The first table that will be used in the dataset is my netflix data table. This table is downloaded from my account on netflix. It is downloaded as a CSV file and contains three different attributes. The first is a categorical, specifically nominal, attribute, it is the name of the TV show/specific episode or movie that I watched. The second is an ordinal attribute, this attribute holds the date that the show or movie was watched. The final attribute for this table is continuous and contains the length of the show or movie that I watched. This attribute was added manually by me after the data was downloaded.  \n",
    "The second table that I want to join with the first is a data from a weather.gov website. This dataset is a csv file downloaded from an government website: https://w2.weather.gov/climate/xmacis.php?wfo=otx. There are three attributes that I will focus on. The first attribute is the date, this is the attribute that I plan to use as the key to merge the two datasets together. The second attribute is an interval attribute and will describe the temperature in degrees fahrenheit for that date. The third and fourth attributes are ratio attributes, with the third holding the amount of precipitatio/snowfall for each date in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Data Preparation\n",
    "Both data sets will be loaded in from their respective csv files in order to begin the data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import py_utils\n",
    "\n",
    "#load in netflix viewing data\n",
    "viewing_df = py_utils.load_data(\"netflix_viewing_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data sets will need to be cleaned before they can be merged. In the weather dataset, the precipitation column will be cleaned by replacing the amount of precipitation with either a 0 or a 1 depending on whether or not there was precipitation on a given day.  \n",
    "\n",
    "In addition to this, the median temperature was found and used to create another category, \"Temp Category\". If the high on a given date is above or equal to the median (73), the date is given a \"High\" label in the \"Temp Category\". If the high on a given date is below the median (73), the date is given a \"Low\" label in the \"Temp Category\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in weather data from MeteoStat API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "api_key = \"x4OYeAd2RgfD0XS6d6it41PRdVLdPE5C\"\n",
    "\n",
    "# meteostat API requires your API key be in the request headers, not in the url query params\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "# step 1: get the spokane weather station\n",
    "url = \"https://api.meteostat.net/v2/stations/search\"\n",
    "# add query params\n",
    "url += \"?query=spokane\"\n",
    "\n",
    "response = requests.get(url=url, headers=headers)\n",
    "json_object = json.loads(response.text)\n",
    "data_object = json_object[\"data\"][0]\n",
    "spokane_id = data_object[\"id\"]\n",
    "\n",
    "\n",
    "# step #2: use spokane weather station id to get daily weather for all of 2020\n",
    "url = \"https://api.meteostat.net/v2/stations/daily\"\n",
    "# add query params\n",
    "url += \"?station=\"\n",
    "url += spokane_id\n",
    "url += \"&start=2020-08-29\"\n",
    "url += \"&end=2020-10-30\"\n",
    "\n",
    "response = requests.get(url=url, headers=headers)\n",
    "json_object = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-769ebed0e396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_object\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snow\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdaily_prec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtavg_ser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprec_ser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdaily_prec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xx' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create the necessary pandas object to hold the data\n",
    "data_object = json_object[\"data\"]\n",
    "#weather_df = py_utils.load_weather(data_object)\n",
    "\n",
    "weather_df = pd.DataFrame()\n",
    "tavg_ser = pd.Series(dtype=float)\n",
    "prec_ser = pd.Series(dtype=float)\n",
    "\n",
    "# load in the data \n",
    "for date_object in data_object:\n",
    "    date = date_object[\"date\"]\n",
    "    tavg = date_object[\"tavg\"] * (9 / 5) + 32 #convert to F\n",
    "    prec = date_object[\"prcp\"]\n",
    "    snow = date_object[\"snow\"]\n",
    "    daily_prec = float(prec) + float(snow)\n",
    "    tavg_ser[date] = xx\n",
    "    prec_ser[date] = daily_prec\n",
    "\n",
    "# put the data into the dataframe\n",
    "weather_df[\"tavg\"] = tavg_ser\n",
    "weather_df[\"prec\"] = prec_ser\n",
    "weather_df[\"Date\"] = prec_ser.index\n",
    "weather_df.set_index(\"Date\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean precipitation column\n",
    "precip_ser = weather_df[\"prec\"]\n",
    "\n",
    "# if rain then 1, if not then 0\n",
    "for i in range (0, len(precip_ser)):\n",
    "    if precip_ser[i] > 0:\n",
    "        precip_ser.replace(precip_ser[i], 1, inplace=True)\n",
    "weather_df[\"prec\"] = precip_ser\n",
    "\n",
    "# find median for future split\n",
    "temp_ser = weather_df[\"tavg\"]\n",
    "temp_ser.sort_values()\n",
    "temp_median = temp_ser.median()\n",
    "print(\"Median temperature:\", temp_median)\n",
    "\n",
    "# add column to keep track of low/high\n",
    "temp_ser = weather_df[\"tavg\"] # get the unsorted series\n",
    "low_high_ser = pd.Series(dtype=int)\n",
    "for i in range (0, len(temp_ser)):\n",
    "    if temp_ser[i] < temp_median:\n",
    "        low_high_ser = low_high_ser.append(pd.Series(\"Low\"))\n",
    "    elif temp_ser[i] == temp_median or temp_ser[i] > temp_median:\n",
    "        low_high_ser = low_high_ser.append(pd.Series(\"High\"))\n",
    "        \n",
    "weather_df[\"temp cat\"] = low_high_ser.values\n",
    "weather_df.to_csv(\"spokane_weather_clean_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The netflix dataset also needs to be cleaned. Rather than add the current DataFrame which includes each individual show title and watch time, I will create a new dataframe that only includes the date and the total watch time for that particular date. This new dataframe will be the one merged with the Spokane weather dataframe that was cleaned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = viewing_df[\"Date\"] #get date column\n",
    "time_col = viewing_df[\"Length\"] #get length column\n",
    "total_watch_df = pd.DataFrame() #create new df\n",
    "\n",
    "# create new series to add to df\n",
    "dates_no_repeats = pd.Series(dtype=int)\n",
    "total_watch_ser = pd.Series(dtype=int)\n",
    "\n",
    "# add each date to the dates_no_repeats sereis\n",
    "for i in range (8, 11):\n",
    "    for j in range(1,32):\n",
    "        if i == 8 and j >= 29:\n",
    "            date_str = \"2020-0\" + str(i) + \"-\" + str(j)\n",
    "            dates_no_repeats = dates_no_repeats.append(pd.Series(date_str))\n",
    "        elif i == 9:\n",
    "            if j < 31: \n",
    "                if j < 10:\n",
    "                    date_str = \"2020-0\" + str(i) + \"-0\" + str(j)\n",
    "                else: \n",
    "                    date_str = \"2020-0\" + str(i) + \"-\" + str(j)\n",
    "                dates_no_repeats = dates_no_repeats.append(pd.Series(date_str))\n",
    "        elif i == 10:\n",
    "            if j < 31: \n",
    "                if j < 10:\n",
    "                    date_str = \"2020-\" + str(i) + \"-0\" + str(j)\n",
    "                else: \n",
    "                    date_str = \"2020-\" + str(i) + \"-\" + str(j)\n",
    "                dates_no_repeats = dates_no_repeats.append(pd.Series(date_str))\n",
    "\n",
    "date_col = dates_no_repeats.values\n",
    "total_watch_df[\"Date\"] = dates_no_repeats.values # add to the df\n",
    "date_ser = total_watch_df[\"Date\"] # get the column from the df \n",
    "\n",
    "# add 0s to each value in total_watch_ser\n",
    "for i in range (len(total_watch_df[\"Date\"])):\n",
    "    total_watch_ser = total_watch_ser.append(pd.Series(0))\n",
    "\n",
    "total_watch_df[\"Viewing Time\"] = total_watch_ser.values # add to the df\n",
    "watch_ser = total_watch_df[\"Viewing Time\"].copy() # get the column for use below\n",
    "\n",
    "# add total times to each date\n",
    "for i in range(len(date_col)):\n",
    "    date = date_col[i] # get date from old df\n",
    "    for j in range(0, len(total_watch_df[\"Date\"])):\n",
    "        target_date = date_ser[j] # target date from new df\n",
    "        if date == target_date:\n",
    "            new_num = watch_ser[j] + time_col[i] # add current value to new value for each date\n",
    "            watch_ser[j] = new_num # set equal to new value\n",
    "            #total_watch_df.loc[\"Viewing time\", j] = new_num\n",
    "            \n",
    "total_watch_df[\"Viewing Time\"] = watch_ser.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the two datasets are cleaned, they need to be merged. In the code cell below, the weather_df that contains the date, average temperature, precipation, and temp category will be merged with the total_watch_df that contains the date and total viewing time for that date. These two dataframes will be merged on the \"Date\" category and written to a csv file named \"merged_netflix_viewing_weather.csv\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two datasets together\n",
    "merged_netflix_weather_df = total_watch_df.merge(weather_df, on=\"Date\")\n",
    "merged_netflix_weather_df.to_csv(\"merged_netflix_viewing_weather.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Exploratory Data Analysis\n",
    "Before moving into more complex data aggregation techniques, I want to first show the initial relationship between the temperature and the number of minutes watched. This will be done directly below using a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot showing the initial relationship between the two data sets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Viewing Time vs. Temperature\")\n",
    "plt.scatter(merged_netflix_weather_df[\"tavg\"], merged_netflix_weather_df[\"Viewing Time\"], s=20)\n",
    "plt.xlabel(\"Temperature (F)\")\n",
    "plt.ylabel(\"Minutes watched\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = merged_netflix_weather_df[\"prec\"]\n",
    "time = merged_netflix_weather_df[\"Viewing Time\"]\n",
    "prec_ser = pd.Series(dtype=float)\n",
    "no_prec_ser = pd.Series(dtype=float)\n",
    "for i in range(len(merged_netflix_weather_df[\"prec\"])):\n",
    "    if prec[i] == 1.0:\n",
    "        prec_ser = prec_ser.append(pd.Series(time[i]))\n",
    "    elif prec[i] == 0.0:\n",
    "        no_prec_ser = no_prec_ser.append(pd.Series(time[i]))\n",
    "plt.bar(0,no_prec_ser.mean())\n",
    "plt.bar(1, prec_ser.mean())\n",
    "plt.xticks((0, 1), ('Prec', 'No Prec'))\n",
    "plt.ylabel(\"Mean Minutes Watched\")\n",
    "plt.title(\"Viewing Time With Precipitation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot above does not clearly show a relationship between the temperature and the number of minutes watched. In order to get a clearer picture of the meaning behind and the answer to our hypotheses, more data aggregation and relationship tests will need to be computed.\n",
    "\n",
    "The data will first undergo a split based on the \"Temp Category\" column. All of the dates with a \"Low\" category will be gathered into a dataframe while all of the dates with a \"High\" category will be gathered into a different dataframe. After this, the mean and standard deviation will be calculated for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "low_temp_df = pd.DataFrame()\n",
    "high_temp_df = pd.DataFrame()\n",
    "grouped_by_temp_category = merged_netflix_weather_df.groupby(\"temp cat\")\n",
    "for group_name, group_df in grouped_by_temp_category:\n",
    "    if group_name == \"Low\":\n",
    "        low_temp_df = group_df\n",
    "    elif group_name == \"High\":\n",
    "        high_temp_df = group_df\n",
    "\n",
    "# find mean viewing time of both groups\n",
    "viewing_time_low_ser = low_temp_df[\"Viewing Time\"]\n",
    "viewing_time_high_ser = high_temp_df[\"Viewing Time\"]\n",
    "print(\"Mean of low category\", viewing_time_low_ser.mean())\n",
    "print(\"Mean of high category\", viewing_time_high_ser.mean())\n",
    "print()\n",
    "\n",
    "# find the standard deviation of each group\n",
    "print(\"Standard deviation of low category\", viewing_time_low_ser.std())\n",
    "print(\"Standard deviation of high category\", viewing_time_high_ser.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above mean and standard deviation of each temperature category, the confidence intervals will be computed. These confidence intervals will be found at 95%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Confidence Interval at 95%\n",
    "\n",
    "#low weather \n",
    "Xbar_low = viewing_time_low_ser.mean()\n",
    "std_low = viewing_time_low_ser.std()\n",
    "n = len(viewing_time_low_ser)\n",
    "t = 2.000\n",
    "\n",
    "margin_of_error = t * std_low / np.sqrt(n)\n",
    "conf_interval_low_temp = (Xbar_low - margin_of_error, Xbar_low + margin_of_error)\n",
    "print(\"Low temp confidence interval:\", conf_interval_low_temp)\n",
    "\n",
    "#high weather \n",
    "Xbar_high = viewing_time_high_ser.mean()\n",
    "std_high = viewing_time_high_ser.std()\n",
    "n = len(viewing_time_high_ser)\n",
    "t = 2.000\n",
    "\n",
    "margin_of_error = t * std_high / np.sqrt(n)\n",
    "conf_interval_high_temp = (Xbar_high - margin_of_error, Xbar_high + margin_of_error)\n",
    "print(\"High temp confidence interval:\", conf_interval_high_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence intervals for each group have a very large range. In order to better show this relationship, the confidence interval for each will be shown in a graph below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## graph them\n",
    "plt.figure()\n",
    "plt.title(\"Low and High Temp Watching Confidence Intervals\")\n",
    "\n",
    "plt.plot([.8, .8], conf_interval_low_temp, marker=\"_\", color=\"blue\")\n",
    "plt.plot([.8], [Xbar_low], marker=\"o\", color=\"blue\", label=\"Low Temp Viewing Time Mean\")\n",
    "\n",
    "plt.plot([1.2, 1.2], conf_interval_high_temp, marker=\"_\", color=\"red\")\n",
    "plt.plot([1.2], [Xbar_high], marker=\"o\", color=\"red\", label=\"High Temp Viewing Time Mean\")\n",
    "\n",
    "#set x and y limits\n",
    "plt.ylim([10, 30])\n",
    "plt.xlim([0.5, 1.5])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph above, the low temp viewing confidence interval is quite large while the high temp confidence interval has a smaller range. However, both ranges are still quite large. At first glance, it appears that the low temp viewing mean could be higher than the high temp viewing mean. This first glance analysis will be the basis of our first hypothesis test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Hypothesis Testing\n",
    "For this particular statistical hypothesis test, a 2 sample, independent, right tailed test to find out if I watched more netflix when the temperature was below 75 degrees (low category) than when the temperature was greater than or equal to 75 degrees (High category). We will be using a significance level of .05.\n",
    "\n",
    "Step 1:  \n",
    "$H_0: \\mu_1 \\leq \\mu_2$  \n",
    "$H_0: \\mu_1 > \\mu_2$\n",
    "\n",
    "Step 2:  \n",
    "$\\alpha = .05$\n",
    "\n",
    "Step 3:  \n",
    "The appropriate t-test:  \n",
    "$t = \\frac{\\overline{x_1} - \\overline{x_2}}{\\sqrt{S_p^2(\\frac{1}{n_1} + \\frac{1}{n_2})}}$ \n",
    "\n",
    "Step 4:  \n",
    "Find t-critical:  \n",
    "* right tailed\n",
    "* $\\alpha = .05$\n",
    "* ddof = $n_1 + n_2 - 2$ = 31 + 31 - 2 = 60\n",
    "\n",
    "t-critical = 1.671\n",
    "\n",
    "Decision rule:  \n",
    "* If t-computed is > 1.671 then reject $H_0$  \n",
    "* If t-computed is $\\leq$ 1.671 then do not reject $H_0$\n",
    "\n",
    "Step 5:  \n",
    "t-test:  \n",
    "$t = \\frac{\\overline{x_1} - \\overline{x_2}}{\\sqrt{S_p^2(\\frac{1}{n_1} + \\frac{1}{n_2})}}$ \n",
    "\n",
    "In the code cell below the SciPy method ttest_ind() will be used to compute the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "t_value, p_value = stats.ttest_ind(viewing_time_low_ser.tolist(), viewing_time_high_ser.tolist())\n",
    "print(t_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since t-computed (.2913) is less than t-critical (1.671):\n",
    "* Do not reject $H_0$\n",
    "    * $\\mu_1 \\leq \\mu_2$\n",
    "* Cannot accept $H_1$\n",
    "    * $\\mu_1 > \\mu_2$\n",
    "    \n",
    "This conclusion will be checked using the p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_value / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value (.3859) is greater than $\\alpha$ (.05), $H_0$ cannot be rejected.\n",
    "\n",
    "Conclusion: At the .05 level of significance, we can reasonable assume that the mean netflix time watching when the weather is \"low\" (less that 75 degrees) is less than or equal to the time spent watching when the weather is \"high\" (greater than or equal to 60.8 degrees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:  \n",
    "$H_0: \\mu_1 \\leq \\mu_2$  \n",
    "$H_0: \\mu_1 > \\mu_2$\n",
    "\n",
    "Step 2:  \n",
    "$\\alpha = .05$\n",
    "\n",
    "Step 3:  \n",
    "The appropriate t-test:  \n",
    "$t = \\frac{\\overline{x_1} - \\overline{x_2}}{\\sqrt{S_p^2(\\frac{1}{n_1} + \\frac{1}{n_2})}}$ \n",
    "\n",
    "Step 4:  \n",
    "Find t-critical:  \n",
    "* right tailed\n",
    "* $\\alpha = .05$\n",
    "* ddof = $n_1 + n_2 - 2$ = 31 + 31 - 2 = 60\n",
    "\n",
    "t-critical = 1.671\n",
    "\n",
    "Decision rule:  \n",
    "* If t-computed is > 1.671 then reject $H_0$  \n",
    "* If t-computed is $\\leq$ 1.671 then do not reject $H_0$\n",
    "\n",
    "Step 5:  \n",
    "t-test:  \n",
    "$t = \\frac{\\overline{x_1} - \\overline{x_2}}{\\sqrt{S_p^2(\\frac{1}{n_1} + \\frac{1}{n_2})}}$ \n",
    "\n",
    "In the code cell below the SciPy method ttest_ind() will be used to compute the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "t_value, p_value = stats.ttest_ind(prec_ser.tolist(), no_prec_ser.tolist())\n",
    "print(t_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_value / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value (.4536) is greater than $\\alpha$ (.05), $H_0$ cannot be rejected.\n",
    "\n",
    "Conclusion: At the .05 level of significance, we can reasonable assume that the mean netflix time watching when the weather there is precipitation (rain or snow) is not greater than the mean time spent watching netflix on days when there was no netflix. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
